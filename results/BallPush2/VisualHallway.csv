Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
50000,1.93231,-0.060158662,999.0,-1.0000000447034836,-1.0000000447034836,0.0010780717,0.022115538,0.0002842176,0.19473918,0.004737486,1.0
100000,1.8653679,-0.0691812,976.795918367347,-0.7736245335212776,-0.7736245335212776,0.008414812,0.025345441,0.0002559173,0.18530574,0.004266757,1.0
150000,1.802711,-0.07892616,981.7592592592592,-0.8899778229080968,-0.8899778229080968,0.0018984176,0.02226153,0.00022472341,0.1749078,0.003747899,1.0
200000,1.7474879,-0.08199104,979.64,-0.8803600384294987,-0.8803600384294987,0.004115174,0.024220416,0.00019358392,0.16452795,0.0032299452,1.0
250000,1.7098598,-0.08271902,983.0,-0.8817020785443637,-0.8817020785443637,0.0041426425,0.022812322,0.00016267577,0.15422523,0.0027158395,1.0
300000,1.708892,-0.06940542,982.566037735849,-0.7948151336244818,-0.7948151336244818,0.0054411124,0.024499727,0.00013490528,0.14496839,0.0022539233,1.0
350000,1.6913657,-0.069192074,988.28,-0.8892120402306318,-0.8892120402306318,0.008010353,0.02469088,0.00010711521,0.13570502,0.0017916812,1.0
400000,1.6241009,-0.05653028,927.074074074074,0.18322589896895267,0.18322589896895267,0.038513817,0.023063088,7.628744e-05,0.12542912,0.0012789132,1.0
450000,1.5030766,-0.0049809422,909.925925925926,0.10790736800818532,0.10790736800818532,0.042186283,0.024927488,4.5486933e-05,0.115162276,0.0007665978,1.0
500000,1.3232402,0.032607842,776.5384615384615,1.1459045905333298,1.1459045905333298,0.08859231,0.024196886,1.4668416e-05,0.10488945,0.000253983,1.0
