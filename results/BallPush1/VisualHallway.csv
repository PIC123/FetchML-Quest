Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
50000,1.9266887,-0.020360477,982.0408163265306,-0.8810000416575646,-0.8810000416575646,0.009360881,0.02142084,0.0002842305,0.19474351,0.0047377003,1.0
100000,1.8907144,-0.038659796,967.58,-0.6684520260989666,-0.6684520260989666,0.012104035,0.026008671,0.00025625562,0.18541853,0.004272384,1.0
150000,1.7619334,-0.044730682,958.5185185185185,-0.5889259576797485,-0.5889259576797485,0.0121415695,0.023896331,0.00022535547,0.17511848,0.0037584123,1.0
200000,1.6295621,-0.045842804,855.0862068965517,0.5235206764833681,0.5235206764833681,0.052030962,0.022761008,0.00019454767,0.16484918,0.0032459751,1.0
250000,1.4452586,0.13269725,754.3181818181819,1.3257445711355942,1.3257445711355942,0.09294972,0.025060713,0.00016374138,0.15458044,0.0027335635,1.0
300000,1.3669077,0.2361749,686.3013698630137,1.538545939064509,1.538545939064509,0.10234533,0.025050769,0.00013290581,0.14430192,0.0022206656,1.0
350000,1.1441528,0.26962575,752.6153846153846,1.1255937311798334,1.1255937311798334,0.09855453,0.023136862,0.00010517527,0.1350584,0.001759414,1.0
400000,1.1408792,0.20558195,768.5076923076923,1.0455181525850838,1.0455181525850838,0.081625804,0.025502086,7.743571e-05,0.12581187,0.0012980128,1.0
450000,1.0604612,0.20817305,800.5483870967741,0.5698128852152056,0.5698128852152056,0.0571069,0.024520632,4.663641e-05,0.11554544,0.00078571745,1.0
500000,1.0513988,0.19831055,715.072463768116,1.0234608345165632,1.0234608345165632,0.08561347,0.023182556,1.5803857e-05,0.105267905,0.00027286922,1.0
